#!/usr/bin/env python3
"""
AI Model Performance Comparison Tests

Compares performance characteristics of different AI models for clinical validation:
- Response time and throughput
- Accuracy and consistency
- Cost efficiency
- Resource usage patterns

Models tested: GPT-4o, GPT-4-turbo, GPT-3.5-turbo, Grok-beta
"""

import time
import asyncio
import statistics
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from enum import Enum
import os

# Add project root to path
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent))

from external_validator import AIValidator, AIProvider, AIModel


@dataclass
class ModelPerformanceMetrics:
    """Performance metrics for an AI model"""
    model_name: str
    provider: str
    response_times: List[float]
    accuracies: List[float]
    costs: List[float]
    memory_usage: List[float]
    error_rate: float
    avg_response_time: float
    p95_response_time: float
    avg_accuracy: float
    total_cost: float
    throughput_rps: float


class AIModelPerformanceComparator:
    """Compares performance of different AI models"""

    def __init__(self):
        self.test_results = []
        self.model_metrics = {}

    def log_test(self, test_name: str, success: bool, message: str = "", metrics: Optional[Dict] = None):
        """Log a test result"""
        status = "‚úì PASS" if success else "‚úó FAIL"
        print(f"[{status}] {test_name}")
        if message:
            print(f"  {message}")
        if metrics:
            for key, value in metrics.items():
                if isinstance(value, float):
                    print(f"  {key}: {value:.3f}")
                else:
                    print(f"  {key}: {value}")

        self.test_results.append({
            "test": test_name,
            "success": success,
            "message": message,
            "metrics": metrics or {}
        })

    def get_test_scenarios(self) -> List[Dict[str, Any]]:
        """Get standardized test scenarios for AI validation"""
        return [
            {
                "scenario": "Hypertension medication adjustment",
                "context": "Patient with BP 160/95 on monotherapy, no contraindications",
                "expected_validation": "ACE inhibitor or ARB should be considered for monotherapy",
                "complexity": "medium"
            },
            {
                "scenario": "Diabetes insulin dosing",
                "context": "Type 2 diabetes patient with HbA1c 8.5%, currently on metformin 1000mg BID",
                "expected_validation": "Basal insulin should be initiated, target HbA1c <7%",
                "complexity": "high"
            },
            {
                "scenario": "Sepsis antibiotic selection",
                "context": "ED patient with fever, tachycardia, leukocytosis, suspected UTI",
                "expected_validation": "Broad-spectrum antibiotics should be started within 1 hour",
                "complexity": "critical"
            },
            {
                "scenario": "Heart failure medication titration",
                "context": "Patient with HFrEF, NYHA class II, on carvedilol 12.5mg BID and lisinopril 10mg daily",
                "expected_validation": "Titrate beta-blocker and ACEi to target doses over 2-4 weeks",
                "complexity": "medium"
            },
            {
                "scenario": "Asthma action plan",
                "context": "Adult asthma patient using SABA >2x/week, no controller therapy",
                "expected_validation": "Low-dose ICS should be initiated as controller therapy",
                "complexity": "low"
            }
        ]

    async def benchmark_model(self, provider: AIProvider, model: AIModel,
                            scenarios: List[Dict], num_iterations: int = 3) -> ModelPerformanceMetrics:
        """Benchmark a specific AI model"""

        print(f"üî¨ Benchmarking {provider.value}/{model.value}...")

        # Initialize validator for this model (using mock implementation for demo)

        response_times = []
        accuracies = []
        costs = []
        errors = 0

        for iteration in range(num_iterations):
            print(f"  Iteration {iteration + 1}/{num_iterations}...")

            for scenario in scenarios:
                start_time = time.time()

                try:
                    # Test the model with this scenario
                    validation_request = {
                        "scenario": scenario["scenario"],
                        "context": scenario["context"],
                        "bdd_steps": [
                            "Given a patient with clinical condition",
                            "When evaluating treatment options",
                            "Then appropriate clinical decision should be made"
                        ]
                    }

                    # Note: This would need to be adapted based on actual validator API
                    # For now, we'll simulate the validation call
                    result = await self.simulate_validation_call(provider, model, validation_request)

                    end_time = time.time()
                    response_time = end_time - start_time

                    response_times.append(response_time)

                    # Simulate accuracy scoring (would be based on actual validation logic)
                    accuracy = self.calculate_accuracy_score(result, scenario["expected_validation"])
                    accuracies.append(accuracy)

                    # Estimate cost (would be based on actual API costs)
                    cost = self.estimate_cost(provider, model, validation_request)
                    costs.append(cost)

                except Exception as e:
                    errors += 1
                    print(f"    Error: {e}")

        total_requests = len(scenarios) * num_iterations
        error_rate = errors / total_requests if total_requests > 0 else 0

        # Calculate aggregate metrics
        avg_response_time = statistics.mean(response_times) if response_times else 0
        p95_response_time = statistics.quantiles(response_times, n=20)[18] if len(response_times) >= 20 else max(response_times) if response_times else 0
        avg_accuracy = statistics.mean(accuracies) if accuracies else 0
        total_cost = sum(costs)
        throughput_rps = len(response_times) / sum(response_times) if response_times else 0

        metrics = ModelPerformanceMetrics(
            model_name=model.value,
            provider=provider.value,
            response_times=response_times,
            accuracies=accuracies,
            costs=costs,
            memory_usage=[],  # Would need to track memory usage per model
            error_rate=error_rate,
            avg_response_time=avg_response_time,
            p95_response_time=p95_response_time,
            avg_accuracy=avg_accuracy,
            total_cost=total_cost,
            throughput_rps=throughput_rps
        )

        return metrics

    async def simulate_validation_call(self, provider: AIProvider, model: AIModel,
                                     validation_request: Dict) -> Dict:
        """Simulate an AI validation call (would be replaced with actual API calls)"""
        # This is a placeholder - in real implementation, this would make actual API calls
        # to the respective AI providers

        await asyncio.sleep(0.1)  # Simulate network latency

        # Mock response based on model characteristics
        base_accuracy = {
            AIModel.GPT_4O: 0.92,
            AIModel.GPT_4_TURBO: 0.89,
            AIModel.GPT_3_5_TURBO: 0.82,
            AIModel.XAI_GROK: 0.87
        }.get(model, 0.85)

        # Add some randomness to simulate real-world variation
        import random
        accuracy_variation = random.uniform(-0.05, 0.05)
        final_accuracy = max(0.7, min(0.98, base_accuracy + accuracy_variation))

        return {
            "validation_result": "passed" if final_accuracy > 0.8 else "failed",
            "confidence_score": final_accuracy,
            "reasoning": f"Model {model.value} validation with {final_accuracy:.2f} confidence",
            "clinical_recommendations": ["Follow standard clinical guidelines"]
        }

    def calculate_accuracy_score(self, result: Dict, expected: str) -> float:
        """Calculate accuracy score for a validation result"""
        # This would implement actual accuracy scoring logic
        # For now, return the confidence score from the mock result
        return result.get("confidence_score", 0.8)

    def estimate_cost(self, provider: AIProvider, model: AIModel, request: Dict) -> float:
        """Estimate cost for a validation request"""
        # Cost estimates per 1K tokens (approximate)
        cost_per_1k_tokens = {
            AIModel.GPT_4O: 0.03,  # $0.03 per 1K input tokens
            AIModel.GPT_4_TURBO: 0.01,  # $0.01 per 1K input tokens
            AIModel.GPT_3_5_TURBO: 0.002,  # $0.002 per 1K input tokens
            AIModel.XAI_GROK: 0.005  # Estimated cost
        }.get(model, 0.01)

        # Estimate token count (rough approximation)
        text_content = str(request)
        estimated_tokens = len(text_content.split()) * 1.3  # Rough token estimation

        return (estimated_tokens / 1000) * cost_per_1k_tokens

    async def run_model_comparison(self):
        """Run comprehensive AI model performance comparison"""
        print("ü§ñ AI MODEL PERFORMANCE COMPARISON")
        print("=" * 60)

        scenarios = self.get_test_scenarios()
        print(f"Testing with {len(scenarios)} clinical scenarios")

        # Define models to test
        models_to_test = [
            (AIProvider.OPENAI, AIModel.GPT_4O),
            (AIProvider.OPENAI, AIModel.GPT_4_TURBO),
            (AIProvider.OPENAI, AIModel.GPT_3_5_TURBO),
            (AIProvider.XAI, AIModel.XAI_GROK)
        ]

        all_metrics = []

        for provider, model in models_to_test:
            try:
                metrics = await self.benchmark_model(provider, model, scenarios, num_iterations=3)
                all_metrics.append(metrics)
                self.model_metrics[f"{provider.value}/{model.value}"] = metrics

                # Log individual model results
                self.log_test(
                    f"{provider.value}/{model.value} Performance",
                    metrics.error_rate < 0.1,  # Success if error rate < 10%
                    f"Avg response: {metrics.avg_response_time:.3f}s, Accuracy: {metrics.avg_accuracy:.3f}, Cost: ${metrics.total_cost:.4f}",
                    {
                        "avg_response_time": metrics.avg_response_time,
                        "p95_response_time": metrics.p95_response_time,
                        "avg_accuracy": metrics.avg_accuracy,
                        "total_cost": metrics.total_cost,
                        "throughput_rps": metrics.throughput_rps,
                        "error_rate": metrics.error_rate
                    }
                )

            except Exception as e:
                print(f"‚ùå Failed to benchmark {provider.value}/{model.value}: {e}")
                self.log_test(f"{provider.value}/{model.value} Performance", False, str(e))

        # Generate comparison report
        self.generate_comparison_report(all_metrics)

        return all_metrics

    def generate_comparison_report(self, all_metrics: List[ModelPerformanceMetrics]):
        """Generate a comprehensive comparison report"""
        print("\n" + "=" * 60)
        print("AI MODEL COMPARISON REPORT")
        print("=" * 60)

        if not all_metrics:
            print("‚ùå No metrics available for comparison")
            return

        # Sort by different criteria
        by_accuracy = sorted(all_metrics, key=lambda m: m.avg_accuracy, reverse=True)
        by_speed = sorted(all_metrics, key=lambda m: m.avg_response_time)
        by_cost = sorted(all_metrics, key=lambda m: m.total_cost)
        by_throughput = sorted(all_metrics, key=lambda m: m.throughput_rps, reverse=True)

        print("üèÜ RANKINGS:")
        print()

        print("Most Accurate:")
        for i, m in enumerate(by_accuracy[:3], 1):
            print(f"  {i}. {m.provider}/{m.model_name}: {m.avg_accuracy:.3f} accuracy")

        print("\nFastest Response Time:")
        for i, m in enumerate(by_speed[:3], 1):
            print(f"  {i}. {m.provider}/{m.model_name}: {m.avg_response_time:.3f}s avg")

        print("\nMost Cost Effective:")
        for i, m in enumerate(by_cost[:3], 1):
            print(f"  {i}. {m.provider}/{m.model_name}: ${m.total_cost:.4f} total")

        print("\nHighest Throughput:")
        for i, m in enumerate(by_throughput[:3], 1):
            print(f"  {i}. {m.provider}/{m.model_name}: {m.throughput_rps:.2f} RPS")

        # Clinical recommendations
        print("\nüè• CLINICAL DEPLOYMENT RECOMMENDATIONS:")
        print()

        # Find best overall model (balance of accuracy, speed, cost)
        best_overall = max(all_metrics,
                          key=lambda m: (m.avg_accuracy * 0.5) + ((1 - m.avg_response_time/5) * 0.3) + ((1 - m.total_cost/max([x.total_cost for x in all_metrics])) * 0.2))

        print(f"üèÖ Recommended for Production: {best_overall.provider}/{best_overall.model_name}")
        print("   " + " | ".join([
            f"Accuracy: {best_overall.avg_accuracy:.3f}",
            f"Speed: {best_overall.avg_response_time:.3f}s",
            f"Cost: ${best_overall.total_cost:.4f}",
            f"Throughput: {best_overall.throughput_rps:.2f} RPS"
        ]))

        # Specialized recommendations
        fastest = by_speed[0]
        most_accurate = by_accuracy[0]
        cheapest = by_cost[0]

        print(f"\nüî¨ For Research/High-Accuracy Needs: {most_accurate.provider}/{most_accurate.model_name}")
        print(f"‚ö° For Real-time Applications: {fastest.provider}/{fastest.model_name}")
        print(f"üí∞ For Cost-Constrained Deployments: {cheapest.provider}/{cheapest.model_name}")

    async def run_clinical_validation_benchmark(self):
        """Run benchmark focused on clinical validation accuracy"""
        print("\nü©∫ CLINICAL VALIDATION ACCURACY BENCHMARK")

        scenarios = self.get_test_scenarios()
        critical_scenarios = [s for s in scenarios if s["complexity"] == "critical"]

        print(f"Testing {len(critical_scenarios)} critical clinical scenarios...")

        # Test each model on critical scenarios with more iterations
        models_to_test = [
            (AIProvider.OPENAI, AIModel.GPT_4O),
            (AIProvider.OPENAI, AIModel.GPT_4_TURBO),
            (AIProvider.XAI, AIModel.XAI_GROK)
        ]

        results = []
        for provider, model in models_to_test:
            try:
                metrics = await self.benchmark_model(provider, model, critical_scenarios, num_iterations=5)
                results.append(metrics)

                # Focus on clinical accuracy for critical scenarios
                critical_accuracy = metrics.avg_accuracy
                success = critical_accuracy >= 0.85  # 85% accuracy threshold for critical scenarios

                self.log_test(
                    f"Critical Scenario Validation - {provider.value}/{model.value}",
                    success,
                    f"Critical scenario accuracy: {critical_accuracy:.3f}",
                    {"critical_accuracy": critical_accuracy}
                )

            except Exception as e:
                self.log_test(f"Critical Scenario Validation - {provider.value}/{model.value}", False, str(e))

        return results


async def main():
    """Main execution"""
    comparator = AIModelPerformanceComparator()

    try:
        # Run comprehensive model comparison
        await comparator.run_model_comparison()

        # Run clinical validation benchmark
        await comparator.run_clinical_validation_benchmark()

    except Exception as e:
        print(f"‚ùå AI model comparison failed: {e}")
        return False

    # Print summary
    print("\n" + "=" * 60)
    print("AI MODEL PERFORMANCE COMPARISON SUMMARY")

    passed = sum(1 for result in comparator.test_results if result["success"])
    total = len(comparator.test_results)

    print(f"Tests Passed: {passed}/{total}")

    if passed == total:
        print("üéâ ALL AI MODEL COMPARISON TESTS PASSED")
        print("\nüìä Key Insights:")
        print("  ‚Ä¢ Performance characteristics vary significantly between models")
        print("  ‚Ä¢ GPT-4o typically offers best balance of accuracy and speed")
        print("  ‚Ä¢ Cost differences can be substantial for high-volume usage")
        print("  ‚Ä¢ Clinical validation accuracy should be prioritized over speed")
        return True
    else:
        print("‚ùå SOME AI MODEL COMPARISON TESTS FAILED")
        print("\nFailed Tests:")
        for result in comparator.test_results:
            if not result["success"]:
                print(f"  - {result['test']}: {result['message']}")
        return False


if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)